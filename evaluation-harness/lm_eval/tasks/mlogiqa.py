import numpy as np
from lm_eval.base import rf, Task
from lm_eval.metrics import mean
from lm_eval.utils import InstructionTemplates

class MLogiQA(Task):
    VERSION = 0
    DATASET_PATH = "/home/export/base/ycsc_chenkh/hitici_02/online1/data/eval_data/P-MMEval"
    DATASET_NAME = "MLogiQA"


    ORCA_SYSTEM = (
        "You should describe the task and explain your answer. "
        "While answering a multiple choice question, first output the correct answer(s). "
        "Then explain why other answers are wrong. Think like you are answering to a five year old."
    )
    _INSTRUCTIONS_LOGIQA = "Passage: {passage}\nQuestion: {question}\nChoices:\nA. {choice1}\nB. {choice2}\nC. {choice3}\nD. {choice4}\nAnswer:"

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc, instruction_template=None):
        if instruction_template:
            return self.doc_to_text_with_instruction(doc, instruction_template)
        else:
            raise NotImplementedError

    def doc_to_text_with_instruction(self, doc, instruction_template="base"):
        choice_dict = {f"choice{i + 1}": choice for i, choice in enumerate(doc['options'])}
        format_dict = {"question": doc['question'], "passage": doc['context']}
        format_dict.update(choice_dict)
        instruction = self._INSTRUCTIONS_LOGIQA.format_map(format_dict)
        if instruction_template == "orca":
            template = InstructionTemplates.get_template(instruction_template)
            return template.format(
                system_message=self.ORCA_SYSTEM,
                user_message=instruction,
            )
        elif instruction_template == "base":
            return instruction
        else:
            raise ValueError(
                f"Unknown instruction template: {instruction_template}")

    def doc_to_target(self, doc, instruction_template="base"):
        if instruction_template:
            return self.doc_to_target_with_instruction(doc, instruction_template)
        else:
            raise NotImplementedError

    def doc_to_target_with_instruction(self, doc, instruction_template):
        correct_choice = f"Choice {chr(ord('A') + doc['label'])}."
        return correct_choice

    # def construct_requests(self, doc, ctx, instruction_template="base"):
    #     if instruction_template:
    #         choice1 = 'Choice A.'
    #         choice2 = 'Choice B.'
    #         choice3 = 'Choice C.'
    #         choice4 = 'Choice D.'
    #     else:
    #         raise NotImplementedError

    #     ll_choice1, _ = rf.loglikelihood(ctx, choice1)
    #     ll_choice2, _ = rf.loglikelihood(ctx, choice2)
    #     ll_choice3, _ = rf.loglikelihood(ctx, choice3)
    #     ll_choice4, _ = rf.loglikelihood(ctx, choice4)

    #     return ll_choice1, ll_choice2, ll_choice3, ll_choice4

    # def process_results(self, doc, results):
    #     gold = doc["label"]
    #     pred = np.argmax(results)
    #     acc = 1.0 if pred == gold else 0.0
    #     return {"acc": acc}

    def construct_requests(self, doc, ctx, instruction_template=None):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        return rf.greedy_until(ctx, {"until": ['</s>', '\n']})

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        gold = f"Choice {chr(ord('A') + doc['label'])}."
        completion = results[0]
        acc = 1.0 if gold in completion else 0.0
        return {"acc": acc}

    def higher_is_better(self):
        return {"acc": True}

    def aggregation(self):
        return {"acc": mean}

    @staticmethod
    def convert_choice(choice):
        return choice[0].lower() + choice[1:]
    

class MLogiQA_ar(MLogiQA):  # AR
    DATASET_NAME = "mlogiqa-ar"

class MLogiQA_en(MLogiQA):  # EN
    DATASET_NAME = "mlogiqa-en"

class MLogiQA_es(MLogiQA):  # ES
    DATASET_NAME = "mlogiqa-es"

class MLogiQA_fr(MLogiQA):  # FR
    DATASET_NAME = "mlogiqa-fr"

class MLogiQA_ja(MLogiQA):  # JA
    DATASET_NAME = "mlogiqa-ja"

class MLogiQA_ko(MLogiQA):  # KO
    DATASET_NAME = "mlogiqa-ko"

class MLogiQA_pt(MLogiQA):  # PT
    DATASET_NAME = "mlogiqa-pt"

class MLogiQA_th(MLogiQA):  # TH
    DATASET_NAME = "mlogiqa-th"

class MLogiQA_vi(MLogiQA):  # VI
    DATASET_NAME = "mlogiqa-vi"

class MLogiQA_zh(MLogiQA):  # ZH
    DATASET_NAME = "mlogiqa-zh"

LANG_CLASSES = [
    MLogiQA_ar, MLogiQA_en, MLogiQA_es, MLogiQA_fr, MLogiQA_ja, 
    MLogiQA_ko, MLogiQA_pt, MLogiQA_th, MLogiQA_vi, MLogiQA_zh,
]


LANGS = """
ar  en  es  fr  ja  ko  pt  th  vi  zh
""".split()


def construct_tasks():
    tasks = {}
    for lang, lang_class in zip(LANGS, LANG_CLASSES):
        tasks[f"mlogiqa_{lang}"] = lang_class
    return tasks
